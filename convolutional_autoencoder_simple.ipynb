{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    " ---\n",
    " ---  Simple Auto Encoder with convolutional layer for encoding part and inverse convolutional layer for decoding part\n",
    " ---  2018, AliSaaalehi@gmail.com \n",
    " ---  tensorflow implemnetation\n",
    " ---  tensorboard to look inside the network\n",
    " ---  MNIST dataset is used\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-57b849a5ebe7>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f38d0f77890>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data and plot one of the images\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_DATA', one_hot=True, validation_size=0)\n",
    "img = mnist.train.images[0]\n",
    "plt.imshow(img.reshape((28, 28)), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utitlity functions\n",
    "\n",
    "def put_kernels_on_grid (kernel, pad = 1):\n",
    "    '''\n",
    "    This function from: https://gist.github.com/kukuruza/03731dc494603ceab0c5\n",
    "    \n",
    "    Visualize conv. filters as an image (mostly for the 1st layer).\n",
    "    Arranges filters into a grid, with some paddings between adjacent filters.\n",
    "    Args:\n",
    "    kernel:            tensor of shape [Y, X, NumChannels, NumKernels]\n",
    "    pad:               number of black pixels around each filter (between them)\n",
    "    Return:\n",
    "    Tensor of shape [1, (Y+2*pad)*grid_Y, (X+2*pad)*grid_X, NumChannels].\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # get shape of the grid. NumKernels == grid_Y * grid_X\n",
    "    def factorization(n):\n",
    "        for i in range(int(sqrt(float(n))), 0, -1):\n",
    "            if n % i == 0:\n",
    "                if i == 1: print('Who would enter a prime number of filters')\n",
    "                return (i, int(n / i))\n",
    "\n",
    "        \n",
    "    (grid_Y, grid_X) = factorization (kernel.get_shape()[3].value)\n",
    "    #print ('grid: %d = (%d, %d)' % (kernel.get_shape()[3].value, grid_Y, grid_X))\n",
    "\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "    kernel = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # pad X and Y\n",
    "    x = tf.pad(kernel, tf.constant( [[pad,pad],[pad, pad],[0,0],[0,0]] ), mode = 'CONSTANT')\n",
    "\n",
    "    # X and Y dimensions, w.r.t. padding\n",
    "    Y = kernel.get_shape()[0] + 2 * pad\n",
    "    X = kernel.get_shape()[1] + 2 * pad\n",
    "\n",
    "    channels = kernel.get_shape()[2]\n",
    "\n",
    "    # put NumKernels to the 1st dimension\n",
    "    x = tf.transpose(x, (3, 0, 1, 2))\n",
    "    # organize grid on Y axis\n",
    "    x = tf.reshape(x, tf.stack([grid_X, Y * grid_Y, X, channels]))\n",
    "\n",
    "    # switch X and Y axes\n",
    "    x = tf.transpose(x, (0, 2, 1, 3))\n",
    "    # organize grid on X axis\n",
    "    x = tf.reshape(x, tf.stack([1, X * grid_X, Y * grid_Y, channels]))\n",
    "\n",
    "    # back to normal order (not combining with the next step for clarity)\n",
    "    x = tf.transpose(x, (2, 1, 3, 0))\n",
    "\n",
    "    # to tf.image_summary order [batch_size, height, width, channels],\n",
    "    #   where in this case batch_size == 1\n",
    "    x = tf.transpose(x, (3, 0, 1, 2))\n",
    "\n",
    "    # scaling to [0, 255] is not necessary for tensorboard\n",
    "    return x\n",
    "\n",
    "\n",
    "def variable_summaries(var):\n",
    "    ''' Attach a lot of summaries to a Tensor (for TensorBoard visualization) '''\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer functions\n",
    "\n",
    "def conv_layer(input, name, kshape, strides=[1, 1, 1, 1]):\n",
    "    with tf.variable_scope(name):\n",
    "        with tf.name_scope('W'):\n",
    "            W = tf.get_variable(name='W', shape=kshape,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "            variable_summaries(W)\n",
    "        with tf.name_scope('bias'):\n",
    "            b = tf.get_variable(name='b', shape=kshape[3],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "            variable_summaries(b)\n",
    "        out = tf.nn.conv2d(input, W, strides=strides, padding='SAME')\n",
    "        out = tf.nn.bias_add(out, b)\n",
    "        out = tf.nn.relu(out)\n",
    "        \n",
    "        # different activations for different filters are in the different channels of the out tensor,\n",
    "        #    by putting them in the first dimention, we can pass them as images to tf.summary.image()\n",
    "        activations = tf.transpose(out, perm=[3, 1, 2, 0])\n",
    "        # if the bach size is more than one, just put the activations for the first input image in the summary file\n",
    "        activations = activations[:, :, :, 0:1]\n",
    "        tf.summary.image('activations', activations, 20)\n",
    "        \n",
    "        tf.summary.histogram('activations', out)\n",
    "        \n",
    "        grid = put_kernels_on_grid (W[:, :, 0:1, :])\n",
    "        tf.summary.image('kernels', grid, max_outputs=1)    \n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "def p_deconv_layer (input, num_outputs, kshape, stride, name,\n",
    "                    init_w=tf.contrib.layers.xavier_initializer_conv2d(uniform=False),\n",
    "                    init_b=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                    dtype=tf.float32, data_format=\"NHWC\", padding='SAME'):\n",
    "    # unlike the deconv_layer(), here I define the weigh and bias and pass them to\n",
    "    #   the tf.nn.conv2d_transpose()\n",
    "    \n",
    "    filter_size_h, filter_size_w = kshape\n",
    "    stride_h = stride[1]\n",
    "    stride_w = stride[2]\n",
    "    with tf.variable_scope(name):\n",
    "        #calculation of the output_shape:\n",
    "        if data_format == \"NHWC\":\n",
    "            input_channel_size = input.get_shape().as_list()[3]\n",
    "            input_size_h = input.get_shape().as_list()[1]\n",
    "            input_size_w = input.get_shape().as_list()[2]\n",
    "            stride_shape = [1, stride_h, stride_w, 1]\n",
    "            if padding == 'VALID':\n",
    "                output_size_h = (input_size_h - 1)*stride_h + filter_size_h\n",
    "                output_size_w = (input_size_w - 1)*stride_w + filter_size_w\n",
    "            elif padding == 'SAME':\n",
    "                output_size_h = (input_size_h *stride_h)\n",
    "                output_size_w = (input_size_w *stride_w)\n",
    "            else:\n",
    "                raise ValueError(\"unknown padding\")\n",
    "            output_shape = tf.stack([tf.shape(input)[0], \n",
    "                                output_size_h, output_size_w, \n",
    "                                num_outputs])\n",
    "        elif data_format == \"NCHW\":\n",
    "            input_channel_size = input.get_shape().as_list()[1]\n",
    "            input_size_h = input.get_shape().as_list()[2]\n",
    "            input_size_w = input.get_shape().as_list()[3]\n",
    "            stride_shape = [1, 1, stride_h, stride_w]\n",
    "            if padding == 'VALID':\n",
    "                output_size_h = (input_size_h - 1)*stride_h + filter_size_h\n",
    "                output_size_w = (input_size_w - 1)*stride_w + filter_size_w\n",
    "            elif padding == 'SAME':\n",
    "                output_size_h = (input_size_h - 1)*stride_h + 1\n",
    "                output_size_w = (input_size_w - 1)*stride_w + 1\n",
    "            else:\n",
    "                raise ValueError(\"unknown padding\")\n",
    "            output_shape = tf.stack([tf.shape(input)[0], \n",
    "                                    output_size_h, output_size_w, num_outputs])\n",
    "        else:\n",
    "            raise ValueError(\"unknown data_format\")\n",
    "\n",
    "        #creating weights:\n",
    "        shape = [filter_size_h, filter_size_w, \n",
    "               num_outputs, input_channel_size]\n",
    "        W_upconv = tf.get_variable(\"W\", shape=shape, dtype=dtype,\n",
    "                                 initializer=init_w)\n",
    "        variable_summaries(W_upconv)\n",
    "        grid = put_kernels_on_grid (W_upconv)\n",
    "        tf.summary.image('kernels', grid, max_outputs=1)\n",
    "\n",
    "        shape=[num_outputs]\n",
    "        b_upconv = tf.get_variable(\"b\", shape=shape, dtype=dtype, \n",
    "                                 initializer=init_b)\n",
    "        variable_summaries(b_upconv)\n",
    "\n",
    "        upconv = tf.nn.conv2d_transpose(input, W_upconv, output_shape, stride_shape,\n",
    "                                      padding=padding,\n",
    "                                      data_format=data_format)\n",
    "        output = tf.nn.bias_add(upconv, b_upconv, data_format=data_format)\n",
    "\n",
    "        #Now output.get_shape() is equal (?,?,?,?) which can become a problem in the \n",
    "        #next layers. This can be repaired by reshaping the tensor to its shape:\n",
    "        output = tf.reshape(output, output_shape)\n",
    "        #now the shape is back to (?, H, W, C) or (?, C, H, W)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def maxpool_2d(input, name, kshape=[1, 2, 2, 1], strides=[1, 2, 2, 1]):\n",
    "    with tf.name_scope(name):\n",
    "        output = tf.nn.max_pool(input,\n",
    "                             ksize=kshape, #size of window\n",
    "                             strides=strides,\n",
    "                             padding='SAME')\n",
    "    return output\n",
    "\n",
    "\n",
    "    \n",
    "def get_simple_autoencoder_graph(input_images, name):\n",
    "    with tf.name_scope(name):\n",
    "        # encoding part\n",
    "        conv1 = conv_layer(input_images, name='conv1', kshape=[5, 5, 1, 20])\n",
    "        \n",
    "        # decoding part\n",
    "        #output_images = deconv_layer(conv1, name='deconv1', kshape=[5, 5], n_outputs=1)\n",
    "        output_images = p_deconv_layer(conv1, num_outputs=1, kshape=[5, 5], stride=[1, 1, 1, 1], name='deconv1')\n",
    "        \n",
    "        tf.summary.image('output_images', output_images, 4)\n",
    "\n",
    "        \n",
    "        with tf.name_scope('cost'):\n",
    "            cost = tf.reduce_mean(tf.square(tf.subtract(output_images, input_images)))\n",
    "        tf.summary.scalar('cost', cost) \n",
    "        \n",
    "        return cost, output_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # creat the log file directory\n",
    "    if tf.gfile.Exists(FLAGS.log_dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    \n",
    "     # to be able to rerun this cell, we need to reset the graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "    def plot_reconstructed(sess, input_images, output_images):\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "        in_imgs = mnist.test.images[:10]\n",
    "        reconstructed = sess.run(output_images, feed_dict={input_images: in_imgs.reshape((10, 28, 28, 1))})\n",
    "\n",
    "        for images, row in zip([in_imgs, reconstructed], axes):\n",
    "            for img, ax in zip(images, row):\n",
    "                ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "        fig.tight_layout(pad=0.1)\n",
    "    \n",
    "    \n",
    "    def feed_dict(train_flag):\n",
    "        #Make a TensorFlow feed_dict: maps data onto Tensor placeholders\n",
    "        if train_flag == True:\n",
    "            X, y = mnist.train.next_batch(FLAGS.batch_size)\n",
    "        else:\n",
    "            X, y = mnist.test.images, mnist.test.labels\n",
    "        images = X.reshape((-1, 28, 28, 1))\n",
    "        return {input_images: images}\n",
    "    \n",
    "\n",
    "    # input and target placeholders\n",
    "    with tf.name_scope('Inputs'):\n",
    "        input_images = tf.placeholder(tf.float32, (None, 28, 28, 1), name=\"input_images\")\n",
    "        tf.summary.image('input_images', input_images, 4)\n",
    "    \n",
    "    cost, output_images = get_simple_autoencoder_graph(input_images, name='my_conv_autoencoder')\n",
    "        \n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate).minimize(cost)\n",
    "\n",
    "   \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test')\n",
    "\n",
    "\n",
    "    #with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    n_batches = int(mnist.train.num_examples / FLAGS.batch_size)\n",
    "    for epoch in range(FLAGS.max_epochs):\n",
    "        avg_cost = 0            \n",
    "\n",
    "        for i in range(n_batches):\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary], feed_dict=feed_dict(FLAGS.train))\n",
    "            avg_cost += c/n_batches\n",
    "\n",
    "        if epoch % 1 ==0:\n",
    "            train_writer.add_summary(summary, epoch)    \n",
    "\n",
    "        print('Epoch', epoch+1, '/', FLAGS.max_epochs, 'cost:', avg_cost)\n",
    "    print('Optimization Finished')\n",
    "\n",
    "    print('cost on test data:', sess.run(cost, feed_dict=feed_dict(not FLAGS.train)))\n",
    "    plot_reconstructed(sess, input_images, output_images)\n",
    "     \n",
    "    return sess, output_images, input_images\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--max_epochs', type=int, default=7,\n",
    "                      help='Number of steps to run trainer.')\n",
    "    parser.add_argument('--batch_size', type=int, default=100,\n",
    "                       help='Batch size.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                      help='Initial learning rate')\n",
    "    parser.add_argument('--train', type=bool, default=True,\n",
    "                       help='Set True for trainign phase and False in testing phase')\n",
    "    parser.add_argument(\n",
    "      '--data_dir',\n",
    "      type=str,\n",
    "      default=os.path.join('.','input_data'),\n",
    "      help='Directory for storing input data')\n",
    "    parser.add_argument(\n",
    "      '--log_dir',\n",
    "      type=str,\n",
    "      default=os.path.join('.',\n",
    "                           'log'),\n",
    "      help='Summaries log directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    #sess = tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)    \n",
    "    sess, output_images, input_images = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do more experiments after training\n",
    "\n",
    "# generate noisy images and pass to the network\n",
    "noise_factor = 0.1\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "in_imgs = mnist.test.images[:10]\n",
    "noisy_imgs = in_imgs + noise_factor * np.random.randn(*in_imgs.shape)\n",
    "noisy_imgs = np.clip(noisy_imgs, 0.0, 1.)\n",
    "\n",
    "reconstructed = sess.run(output_images, feed_dict={input_images: noisy_imgs.reshape((10, 28, 28, 1))})\n",
    "print('reconstructed.shape: ', reconstructed.shape)\n",
    "for images, row in zip([noisy_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "fig.tight_layout(pad=0.1)\n",
    "\n",
    "\n",
    "\n",
    "# look at the exaxt values of trained filters\n",
    "for v in tf.trainable_variables():\n",
    "    print v\n",
    "\n",
    "def get_variable(name):\n",
    "    name += ':0' # the weight names have :0 at their end\n",
    "    var = [v for v in tf.trainable_variables() if v.name==name]\n",
    "    return var[0]\n",
    "\n",
    "# print trained filters from encoding part\n",
    "v = get_variable('conv1/W').eval(session = sess)\n",
    "print(v.shape)\n",
    "#print('First filter is: ', v[:, :, :, 0])\n",
    "\n",
    "# print filter values from the decoding part\n",
    "v = get_variable('deconv1/W').eval(session = sess)\n",
    "print(v.shape)\n",
    "#print('First filter is: ', v[:, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
